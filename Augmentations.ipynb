{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e69740-390a-4ec4-86a7-10fc4ea2ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This file contains the code for generating the train/test/validation sets\n",
    "# and also contains the code for augmenting the train set in various ways,\n",
    "# such as by applying oversampling or by applying random variations.\n",
    "# The train/test/validation sets are generated as CSV files. They are generated from the original spectra.csv\n",
    "# provided by the preprocessing file.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "import random\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################################################\n",
    "                                  # VARIATIONS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def horizontal_shift(x: np.ndarray, y: np.ndarray, max_shift: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift spectrum left/right along the x-axis by interpolating the y-values.\n",
    "    \"\"\"\n",
    "    delta = random.randint(-max_shift, max_shift)\n",
    "    x_shifted = x + delta\n",
    "    f = interp1d(x_shifted, y, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    \n",
    "    return f(x)\n",
    "\n",
    "\n",
    "def vertical_noise(y: np.ndarray, scale: float = 0.05) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add small vertical noise to the spectrum to simulate measurement fluctuations.\n",
    "    \"\"\"\n",
    "    noise = (1 - y) * np.random.uniform(-scale, scale, size=y.shape)\n",
    "    \n",
    "    return np.clip(y + noise, 0, 1)\n",
    "\n",
    "\n",
    "def linear_comb(y_list: List, weights: float = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Combine multiple spectra from the same functional group.\n",
    "    \"\"\"\n",
    "    y_array = np.array(y_list)\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = np.random.dirichlet(np.ones(len(y_list)), size=1)[0]\n",
    "    \n",
    "    return np.dot(weights, y_array)\n",
    "\n",
    "\n",
    "def intensity_scaling(y: np.ndarray, gamma_range=(0.9, 1.1)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly scale the entire spectrum's intensity.\n",
    "    \"\"\"\n",
    "    gamma = np.random.uniform(*gamma_range)\n",
    "    \n",
    "    return gamma * y\n",
    "\n",
    "\n",
    "def sinusoidal_drift(x: np.ndarray, y: np.ndarray, amp_range=(0.005, 0.02), freq_range=(0.5, 2)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a sinusoidal drift to the spectrum to simulate baseline distortions.\n",
    "    \"\"\"\n",
    "    A = np.random.uniform(*amp_range)\n",
    "    x_range = x[-1] - x[0]\n",
    "    if x_range == 0:\n",
    "        f = 0\n",
    "    else:\n",
    "        f = np.random.uniform(*freq_range) / x_range  # normalize over x range\n",
    "    phi = np.random.uniform(0, 2 * np.pi)\n",
    "    drift = A * np.sin(2 * np.pi * f * x + phi)\n",
    "    \n",
    "    return y + drift\n",
    "\n",
    "\n",
    "def smoothing_variation(y: np.ndarray, window_choices=[5, 7, 9, 11], polyorder=2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Smooth the spectrum using a Savitzky-Golay filter with a random window size.\n",
    "    \"\"\"\n",
    "    window_size = np.random.choice(window_choices)\n",
    "    \n",
    "    if len(y) >= window_size:\n",
    "        return savgol_filter(y, window_length=window_size, polyorder=polyorder)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1b6bf92-78ee-4105-b34d-8b963a816f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "                             # AUGMENTING DATASET\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Apply random combinations of augmentations\n",
    "def augment_randomly(df, x, n_aug_per_class):\n",
    "    augmented_spectra = []\n",
    "    augmented_labels = []\n",
    "    origin_ids = [] # keep track of which original spectra we perform the augmentations on\n",
    "\n",
    "    # Group spectra by label for linear combination\n",
    "    label_to_spectra = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        label_str = json.dumps(row['labels'])\n",
    "        label_to_spectra[label_str].append(np.array(row['spectrum']))\n",
    "\n",
    "    # Randomly augment each sample\n",
    "    for idx, row in df.iterrows():\n",
    "        original_y = np.array(row['spectrum'])\n",
    "        label = row['labels']\n",
    "        label_str = json.dumps(label)\n",
    "        origin_id = f\"sample_{idx}\"\n",
    "\n",
    "        augmented_spectra.append(original_y.tolist())\n",
    "        augmented_labels.append(label)\n",
    "        origin_ids.append(origin_id)\n",
    "        \n",
    "        n_aug = 0\n",
    "        for i in range(len(label)):\n",
    "            if (label[i] == 1):\n",
    "                n_aug = n_aug + n_aug_per_class[i]\n",
    "\n",
    "        for _ in range(n_aug):\n",
    "            y_aug = original_y.copy()\n",
    "            \n",
    "            # Apply random augmentations\n",
    "            if random.random() < 0.5:\n",
    "                y_aug = horizontal_shift(x, y_aug)\n",
    "            if random.random() < 0.5:\n",
    "                y_aug = vertical_noise(y_aug)\n",
    "            if random.random() < 0.5:\n",
    "                y_aug = intensity_scaling(y_aug)\n",
    "            if random.random() < 0.5:\n",
    "                y_aug = sinusoidal_drift(x, y_aug)\n",
    "            if random.random() < 0.5:\n",
    "                y_aug = smoothing_variation(y_aug)\n",
    "            \n",
    "            #if random.random() < 0.5 and len(label_to_spectra[label_str]) >= 2:\n",
    "            #    y_pool = label_to_spectra[label_str]\n",
    "            #    sampled = random.sample(y_pool, k=min(3, len(y_pool)))\n",
    "            #    y_aug = linear_comb(sampled)\n",
    "\n",
    "            # Normalize the data and apply baseline correction again\n",
    "            y_aug = y_aug - np.min(y_aug)\n",
    "            if (np.max(y_aug) != 0):\n",
    "                y_aug = y_aug / np.max(y_aug)\n",
    "\n",
    "            augmented_spectra.append(y_aug.tolist())\n",
    "            augmented_labels.append(label)\n",
    "            origin_ids.append(origin_id)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'spectrum': augmented_spectra,\n",
    "        'labels': augmented_labels,\n",
    "        'origin_id': origin_ids\n",
    "    })\n",
    "\n",
    "# Apply individual augmentations\n",
    "def augment_individually(df, x):\n",
    "    AUG_METHODS = ['horizontal_shift', 'vertical_noise', 'intensity_scaling',\n",
    "                   'sinusoidal_drift', 'smoothing_variation', 'linear_comb']\n",
    "\n",
    "    augmented_spectra = []\n",
    "    augmented_labels = []\n",
    "    origin_ids = [] # keep track of which original spectra we perform the augmentations on\n",
    "    augmentation_types = []\n",
    "\n",
    "    # Group spectra by label for linear combination\n",
    "    label_to_spectra = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        label_str = json.dumps(row['labels'])\n",
    "        label_to_spectra[label_str].append(np.array(row['spectrum']))\n",
    "\n",
    "    # Apply all augmentations individually to each sample\n",
    "    for idx, row in df.iterrows():\n",
    "        original_y = np.array(row['spectrum'])\n",
    "        label = row['labels']\n",
    "        label_str = json.dumps(label)\n",
    "        origin_id = f\"sample_{idx}\"\n",
    "\n",
    "        augmented_spectra.append(original_y.tolist())\n",
    "        augmented_labels.append(label)\n",
    "        origin_ids.append(origin_id)\n",
    "        augmentation_types.append('none')\n",
    "\n",
    "        for method in AUG_METHODS:\n",
    "            y_aug = original_y.copy()\n",
    "\n",
    "            if method == 'horizontal_shift':\n",
    "                y_aug = horizontal_shift(x, y_aug)\n",
    "            elif method == 'vertical_noise':\n",
    "                y_aug = vertical_noise(y_aug)\n",
    "            elif method == 'intensity_scaling':\n",
    "                y_aug = intensity_scaling(y_aug)\n",
    "            elif method == 'sinusoidal_drift':\n",
    "                y_aug = sinusoidal_drift(x, y_aug)\n",
    "            elif method == 'smoothing_variation':\n",
    "                y_aug = smoothing_variation(y_aug)\n",
    "            #elif method == 'linear_comb':\n",
    "            #    y_pool = label_to_spectra[label_str]\n",
    "            #    if len(y_pool) >= 2:\n",
    "            #        sampled = random.sample(y_pool, k=min(3, len(y_pool)))\n",
    "            #        y_aug = linear_comb(sampled)\n",
    "            #    else:\n",
    "            #        continue\n",
    "\n",
    "            # Normalize the data and apply baseline correction again\n",
    "            y_aug = y_aug - np.min(y_aug)\n",
    "            if (np.max(y_aug) != 0):\n",
    "                y_aug = y_aug / np.max(y_aug)\n",
    "\n",
    "            augmented_spectra.append(y_aug.tolist())\n",
    "            augmented_labels.append(label)\n",
    "            origin_ids.append(origin_id)\n",
    "            augmentation_types.append(method)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'spectrum': augmented_spectra,\n",
    "        'labels': augmented_labels,\n",
    "        'origin_id': origin_ids,\n",
    "        'augmentation_type': augmentation_types\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f87b1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "                         # VISUALIZING AUGMENTATIONS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def plot_combined_augmentations(df_aug, id_number):    \n",
    "    example_id = df_aug['origin_id'].iloc[id_number]\n",
    "    example_rows = df_aug[df_aug['origin_id'] == example_id].reset_index(drop=True)\n",
    "    \n",
    "    original_y = np.array(example_rows.iloc[0]['spectrum'])\n",
    "    augmented_y_list = [np.array(row['spectrum']) for _, row in example_rows.iloc[1:].iterrows()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, original_y, label='Original', linewidth=2)\n",
    "    for i, y_aug in enumerate(augmented_y_list):\n",
    "        plt.plot(x, y_aug, label=f'Augmented #{i+1}', alpha=0.7)\n",
    "    plt.xlabel('Wavenumber (cm⁻¹)')\n",
    "    plt.ylabel('Absorbance')\n",
    "    plt.title(f'Origin ID: {example_id} - Original vs Combined Augmentations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_individual_augmentations(df_aug, id_number):    \n",
    "    example_id = df_aug['origin_id'].iloc[id_number]\n",
    "    example_rows = df_aug[df_aug['origin_id'] == example_id].reset_index(drop=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for _, row in example_rows.iterrows():\n",
    "        spectrum = np.array(row['spectrum'])\n",
    "        method = row['augmentation_type']\n",
    "        lw = 2 if method == 'none' else 1\n",
    "        plt.plot(x, spectrum, label=method, linewidth=lw, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Wavenumber (cm⁻¹)')\n",
    "    plt.ylabel('Absorbance')\n",
    "    plt.title(f'Origin ID: {example_id} - Original vs Individual Augmentations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2e9ec6-e30f-4d8d-bd8e-f724c16c9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "                             # OVERSAMPLING\n",
    "################################################################################\n",
    "\n",
    "# Code for mlsmote taken from https://github.com/theopsall/multiSmote\n",
    "from multiSmote.multi_smote import MultiSmote as mlsmote\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Oversampling Technique) to generate synthetic samples for minority classes to combat class imbalance.\n",
    "def apply_SMOTE(df):\n",
    "    spectra = np.stack(df['spectrum'].to_numpy())\n",
    "    labels = np.stack(df['labels'].to_numpy())\n",
    "    \n",
    "    smote = mlsmote()\n",
    "    spectra_oversampled, labels_oversampled = smote.multi_smote(spectra, labels)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'spectrum': [x.tolist() for x in spectra_oversampled], # pandas can't convert numpy arrays to CSVs\n",
    "        'labels': [y.tolist() for y in labels_oversampled]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5958228-752f-4105-bb9c-c7bfbad42fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "                        # TRAIN/TEST/VALIDATION SPLIT\n",
    "################################################################################\n",
    "\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "label_map = [\n",
    "    'phenol',\n",
    "    'aldehyde',\n",
    "    'arene'\n",
    "]\n",
    "\n",
    "\n",
    "def fair_split(df, train_size, validation_size):\n",
    "    # Counting how many spectra contain each group (just for logging)\n",
    "    counts = {}\n",
    "    for i in range(len(label_map)):\n",
    "        counts[label_map[i]] = df['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "    print('Spectra counts per group:')\n",
    "    for name, cnt in counts.items():\n",
    "        print(f'{name}: {cnt}')\n",
    "\n",
    "    try:\n",
    "        # Stage 1 (stratifier): (1 - TRAIN_SIZE)% for “remainder”, TRAIN_SIZE% for training\n",
    "        strat1 = MultilabelStratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=1 - train_size,    # (1 - TRAIN_SIZE)% goes to df_remain\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        Y = np.stack(df['labels'].values)\n",
    "        \n",
    "        # Split the dataset into a train set and a remainder\n",
    "        train_idx, remain_idx = next(strat1.split(df, Y))\n",
    "        df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "        Y_train  = Y[train_idx]\n",
    "        df_remain = df.iloc[remain_idx].reset_index(drop=True)\n",
    "        Y_remain  = Y[remain_idx]\n",
    "\n",
    "        print(f\"\\nAfter Stage 1 → Train: {len(df_train)} (≈{train_size * 100.0}%), Remainder: {len(df_remain)} (≈{(1 - train_size) * 100.0}%)\")\n",
    "\n",
    "        strat2 = MultilabelStratifiedShuffleSplit(\n",
    "            n_splits=1,\n",
    "            test_size=validation_size,   # VALIDATION_SIZE% of df_remain → validation\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "        # Split the remainder into a validation and test set\n",
    "        remain_train_idx, remain_test_idx = next(strat2.split(df_remain, Y_remain))\n",
    "\n",
    "        df_test  = df_remain.iloc[remain_train_idx].reset_index(drop=True)\n",
    "        df_val   = df_remain.iloc[remain_test_idx].reset_index(drop=True)\n",
    "\n",
    "        print(f\"After Stage 2 → Test: {len(df_test)} (≈{(1 - train_size) * (1 - validation_size) * 100.0}%), Validation: {len(df_val)} (≈{(1 - train_size) * validation_size * 100.0}%)\")\n",
    "\n",
    "    except ImportError:\n",
    "        # Fallback (non‐stratified) splitting if iterstrat is missing\n",
    "        # Stage 1: TRAIN_SIZE% train, (1 - TRAIN_SIZE)% remainder\n",
    "        df_train, df_remain = train_test_split(\n",
    "            dataset.df, \n",
    "            test_size=1 - train_size,\n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(f\"\\nAfter Stage 1 (fallback) → Train: {len(df_train)}, Remainder: {len(df_remain)}\")\n",
    "\n",
    "        # Stage 2: within remainder, do (1 - VALIDATION_SIZE)% for test and VALIDATION_SIZE% for val\n",
    "        df_test, df_val = train_test_split(\n",
    "            df_remain,\n",
    "            test_size=VALIDATION_SIZE,   # VALIDATION_SIZE of “remainder” is the validation set \n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(f\"After Stage 2 (fallback) → Test: {len(df_test)}, Validation: {len(df_val)}\")\n",
    "\n",
    "    print()\n",
    "    for i in range(len(label_map)):\n",
    "        name = label_map[i]\n",
    "        orig_tot = df['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "        train_tot = df_train['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "        test_tot  = df_test['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "        val_tot   = df_val['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "        print(f\"Label = {label_map[i]}\")\n",
    "        print(f\" Overall prevalence: {orig_tot / len(df):.2%}, total: {orig_tot}\")\n",
    "        print(f\"→ Train prevalence: {train_tot / len(df_train):.2%}, total: {train_tot}\")\n",
    "        print(f\"→ Test prevalence: {test_tot / len(df_test):.2%}, total: {test_tot}\")\n",
    "        print(f\"→ Val prevalence: {val_tot / len(df_val):.2%}, total: {val_tot}\\n\")\n",
    "\n",
    "    return df_train, df_test, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c104dc91-6eda-4134-8035-fb0c574d1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "                          # DATA CONVERSION\n",
    "################################################################################\n",
    "\n",
    "import json\n",
    "\n",
    "def csv_to_df(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['spectrum'] = df['spectrum'].apply(json.loads)\n",
    "    df['labels'] = df['labels'].apply(json.loads)\n",
    "    return df\n",
    "\n",
    "def df_to_csv(df, csv_path):\n",
    "    # Don't modify the original dataframe and only consider the spectrum and labels columns\n",
    "    df_clone = df.copy()[['spectrum', 'labels']]\n",
    "    df_clone['spectrum'] = df_clone['spectrum'].apply(lambda L: json.dumps(L))\n",
    "    df_clone['labels'] = df_clone['labels'].apply(lambda L: json.dumps(L))\n",
    "    df_clone.to_csv(csv_path, index=False)\n",
    "\n",
    "def labels_to_multi_hot_vector(labels):\n",
    "    multi_hot_vector = []\n",
    "    for i in range(len(label_map)):\n",
    "        if label_map[i] in labels:\n",
    "            multi_hot_vector.append(1)\n",
    "        else:\n",
    "            multi_hot_vector.append(0)\n",
    "    return multi_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ea32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_CSV = 'spectra.csv'\n",
    "\n",
    "# All of the csv reads/writes are in different cells. This is to make it possible to only read/write one csv without redoing\n",
    "# a lot of work for all the other CSVs.\n",
    "\n",
    "df = csv_to_df(ORIGINAL_CSV)\n",
    "# To start, convert labels from string representations to multi-hot vectors\n",
    "df['labels'] = df['labels'].apply(labels_to_multi_hot_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d605d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we need to generate a new set later we don't have to regenerate the training/test/validation sets\n",
    "# we can just use the old training set\n",
    "df_train = csv_to_df('spectra_train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02c566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectra counts per group:\n",
      "phenol: 711\n",
      "aldehyde: 443\n",
      "arene: 9053\n",
      "\n",
      "After Stage 1 → Train: 11711 (≈75.0%), Remainder: 3904 (≈25.0%)\n",
      "After Stage 2 → Test: 2342 (≈15.0%), Validation: 1562 (≈10.0%)\n",
      "\n",
      "Label = phenol\n",
      " Overall prevalence: 4.55%, total: 711\n",
      "→ Train prevalence: 4.55%, total: 533\n",
      "→ Test prevalence: 4.57%, total: 107\n",
      "→ Val prevalence: 4.55%, total: 71\n",
      "\n",
      "Label = aldehyde\n",
      " Overall prevalence: 2.84%, total: 443\n",
      "→ Train prevalence: 2.83%, total: 332\n",
      "→ Test prevalence: 2.86%, total: 67\n",
      "→ Val prevalence: 2.82%, total: 44\n",
      "\n",
      "Label = arene\n",
      " Overall prevalence: 57.98%, total: 9053\n",
      "→ Train prevalence: 57.98%, total: 6790\n",
      "→ Test prevalence: 57.98%, total: 1358\n",
      "→ Val prevalence: 57.94%, total: 905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into 75% training set, 15% test set, 10% validation set\n",
    "df_train, df_test, df_val = fair_split(df, 0.75, 0.4)\n",
    "\n",
    "# Save test and validation sets since we don't want to apply augmentations to them\n",
    "df_to_csv(df_test, 'spectra_test.csv')\n",
    "df_to_csv(df_val, 'spectra_validation.csv')\n",
    "\n",
    "# Save a 'clean' training set as a base measure\n",
    "df_to_csv(df_train, 'spectra_train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b43dcab5-0bd1-4d38-8d49-212cb341ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in the training set before SMOTE:\n",
      "phenol: 533\n",
      "aldehyde: 332\n",
      "arene: 6790\n",
      "Without SMOTE the training set contains 11711 datapoints\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18280\\2269358615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf_train_smote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_SMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdf_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_smote\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'spectra_train_SMOTE.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18280\\3675219644.py\u001b[0m in \u001b[0;36mapply_SMOTE\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;34m'spectrum'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspectra_oversampled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# pandas can't convert numpy arrays to CSVs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels_oversampled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     })\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18280\\3675219644.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;34m'spectrum'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspectra_oversampled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# pandas can't convert numpy arrays to CSVs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels_oversampled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     })\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to the training set to counteract class imbalance and save the result\n",
    "print('Labels in the training set before SMOTE:')\n",
    "for i in range(len(label_map)):\n",
    "    label_count_before = df_train['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "    print('%s: %i' % (label_map[i], label_count_before))\n",
    "print('Without SMOTE the training set contains %i datapoints\\n\\n' % (len(df_train)))\n",
    "\n",
    "df_train_smote = apply_SMOTE(df_train)\n",
    "df_to_csv(df_train_smote, 'spectra_train_SMOTE.csv')\n",
    "\n",
    "print('Labels in the training set after SMOTE:')\n",
    "for i in range(len(label_map)):\n",
    "    label_count_after = df_train_smote['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "    print('%s: %i' % (label_map[i], label_count_after))\n",
    "print('With oversampling the training set contains %i datapoints (%f%% of training set)' % (len(df_train_smote), (len(df_train_smote) / len(df_train) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17303da6-cda5-44a0-85ed-76c36a71896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in the training set before augmentations:\n",
      "phenol: 533\n",
      "aldehyde: 332\n",
      "arene: 6790\n",
      "Without augmentations the training set contains 11711 datapoints\n",
      "\n",
      "\n",
      "Labels in the training set after augmentations:\n",
      "phenol: 6391\n",
      "aldehyde: 5230\n",
      "arene: 27526\n",
      "With augmentations the training set contains 33539 datapoints (286.388865% of training set)\n"
     ]
    }
   ],
   "source": [
    "# Apply augmentations to the training set without SMOTE and save the result\n",
    "MIN_WAVENUMBER = 400\n",
    "MAX_WAVENUMBER = 4000\n",
    "NUM_POINTS = MAX_WAVENUMBER - MIN_WAVENUMBER # every spectrum should have a y value for every wave number\n",
    "x = np.linspace(MIN_WAVENUMBER, MAX_WAVENUMBER, NUM_POINTS)\n",
    "\n",
    "# When augmentations are applied one at a time, performance generally doesn't improve\n",
    "# Besides, augment_individually tends to generate ridiculously dataframes which take forever to convert to CSVs\n",
    "# and train with\n",
    "#df_train_ind_aug = augment_individually(df_train, x)\n",
    "#df_to_csv(df_train_ind_aug, 'spectra_train_ind_aug.csv')\n",
    "\n",
    "print('Labels in the training set before augmentations:')\n",
    "for i in range(len(label_map)):\n",
    "    label_count_before = df_train['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "    print('%s: %i' % (label_map[i], label_count_before))\n",
    "print('Without augmentations the training set contains %i datapoints\\n\\n' % (len(df_train)))\n",
    "\n",
    "#df_train_rand_aug = augment_randomly(df_train, x, n_aug_per_class=[5, 5, 5])\n",
    "#df_to_csv(df_train_rand_aug, 'spectra_train_rand_aug.csv')\n",
    "\n",
    "df_train_rand_aug_oversampled = augment_randomly(df_train, x, n_aug_per_class=[8, 12, 2])\n",
    "df_to_csv(df_train_rand_aug_oversampled, 'spectra_train_rand_aug_oversampled.csv')\n",
    "\n",
    "print('Labels in the training set after augmentations:')\n",
    "for i in range(len(label_map)):\n",
    "    label_count_after = df_train_rand_aug_oversampled['labels'].apply(lambda L: L[i] == 1).sum()\n",
    "    print('%s: %i' % (label_map[i], label_count_after))\n",
    "print('With augmentations the training set contains %i datapoints (%f%% of training set)' % (len(df_train_rand_aug_oversampled), (len(df_train_rand_aug_oversampled) / len(df_train) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentations to the training set with SMOTE and save the result\n",
    "\n",
    "# Removed for the same reason stated above\n",
    "#df_train_ind_aug_smote = augment_individually(df_train_smote, x)\n",
    "#df_to_csv(df_train_ind_aug_smote, 'spectra_train_ind_aug_smote.csv')\n",
    "\n",
    "# Since the dataset with SMOTE contains more samples the number of augmentations per sample should be brought down a bit\n",
    "# to prevent the file size from exploding\n",
    "df_train_rand_aug_smote = augment_randomly(df_train_smote, x, n_aug=2)\n",
    "df_to_csv(df_train_rand_aug_smote, 'spectra_train_rand_aug_smote.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcb14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of datapoints in each training set:')\n",
    "print('Unaltered training set: %i (%f%% bigger))' % (len(df_train), 0.0))\n",
    "print('Training set with SMOTE applied: %i (%f%% bigger))' % (len(df_train_smote), (len(df_train_smote) / len(df_train)) * 100.0 - 100.0))\n",
    "print('Training set with augmentations applied: %i (%f%% bigger))' % (len(df_train_rand_aug), (len(df_train_rand_aug) / len(df_train)) * 100.0 - 100.0))\n",
    "print('Training set with SMOTE + augmentations applied: %i (%f%% bigger))' % (len(df_train_rand_aug_smote), (len(df_train_rand_aug_smote) / len(df_train)) * 100.0 - 100.0))\n",
    "\n",
    "plot_combined_augmentations(df_train_rand_aug, id_number=1)\n",
    "plot_combined_augmentations(df_train_rand_aug_smote, id_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1fceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

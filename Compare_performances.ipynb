{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4b0943",
   "metadata": {},
   "source": [
    "Example code for comparing the performances of the datasets. To be implemented later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca32b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_individual_augmentations(model, batch_transform, device='cpu'):\n",
    "    df = pd.read_csv(IND_AUGMENTED_CSV)\n",
    "    unique_methods = df['augmentation_type'].unique()\n",
    "\n",
    "    method_metrics = {}\n",
    "\n",
    "    for method in unique_methods:\n",
    "        if method == 'none':\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nEvaluating augmentation method: {method}\")\n",
    "        df_method = df[df['augmentation_type'] == method]\n",
    "        \n",
    "        temp_csv = f'temp_{method}.csv'\n",
    "        df_method.to_csv(temp_csv, index=False)\n",
    "        \n",
    "        temp_dataset = IRDataset(temp_csv)\n",
    "        _, test_loader = create_loaders(temp_dataset)\n",
    "        \n",
    "        metrics = evaluate(model, test_loader, batch_transform, device)\n",
    "        method_metrics[method] = metrics['macro']\n",
    "\n",
    "    return method_metrics\n",
    "\n",
    "def evaluate_dataset(csv_path, model, batch_transform, device='cpu'):\n",
    "    dataset = IRDataset(csv_path)\n",
    "    _, test_loader = create_loaders(dataset)\n",
    "    metrics = evaluate(model, test_loader, batch_transform, device)\n",
    "    return metrics['macro']\n",
    "\n",
    "original_metrics = evaluate_dataset(CSV_PATH, model, batch_transform)\n",
    "augmented_metrics = evaluate_dataset(AUGMENTED_CSV, model, batch_transform)\n",
    "individual_metrics = evaluate_individual_augmentations(model, batch_transform)\n",
    "\n",
    "\n",
    "# Compare performances\n",
    "def plot_metrics_comparison(metrics_dict, title=\"Augmentation Method Comparison\"):\n",
    "    labels = list(next(iter(metrics_dict.values())).keys())\n",
    "    methods = list(metrics_dict.keys())\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.15\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    for i, method in enumerate(methods):\n",
    "        values = [metrics_dict[method][label] for label in labels]\n",
    "        plt.bar(x + i * width, values, width=width, label=method)\n",
    "\n",
    "    plt.xticks(x + width * (len(methods) - 1) / 2, labels, rotation=45)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"augmentation_methods_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "all_metrics = {\n",
    "    'original': original_metrics,\n",
    "    'augmented_full': augmented_metrics,\n",
    "    **individual_metrics\n",
    "\n",
    "\n",
    "plot_metrics_comparison(all_metrics, title=\"Augmentation Methods Comparison\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
